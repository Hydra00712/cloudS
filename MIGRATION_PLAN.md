# 4Ô∏è‚É£ MIGRATION PLAN: LOCAL ‚Üí AZURE

## File-by-File Transformation Guide

### üìÑ FILE 1: `preprocess_clean.py`

#### Current State (LOCAL)
```python
df = pd.read_csv("data/raw/social_media_data.csv")
# Process...
df.to_csv("data/processed/cleaned_data.csv")
pickle.dump(encoders, open("data/processed/encoders.pkl", "wb"))
```

#### Migration Changes Required

| Change | Reason | Implementation |
|--------|--------|-----------------|
| Data input | No local raw/ folder in Azure | Read from `azure-storage-blob` |
| Data output | Store in Azure, not local | Write to Azure Blob Storage |
| Encoders persistence | Share across services | Store in Blob + retrieval in training |
| Logging | Audit trail | Add Application Insights logs |
| Dependencies | New packages | Add to `requirements.txt` |

#### Modified `preprocess_clean.py` (Azure-ready)
```python
from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential
import logging
from azure.monitor.opentelemetry import configure_azure_monitor

# === NEW: Set up Azure logging ===
configure_azure_monitor()
logger = logging.getLogger(__name__)

# === NEW: Azure Blob Client ===
credential = DefaultAzureCredential()
blob_service = BlobServiceClient(
    account_url="https://{STORAGE_ACCOUNT}.blob.core.windows.net",
    credential=credential
)
raw_container = blob_service.get_container_client("raw-data")
processed_container = blob_service.get_container_client("processed-data")

# === MODIFIED: Read from Azure Blob ===
blob_client = raw_container.get_blob_client("social_media_data.csv")
with open("temp_raw.csv", "wb") as f:
    f.write(blob_client.download_blob().readall())
df = pd.read_csv("temp_raw.csv")
logger.info(f"Loaded {len(df)} rows from Azure Blob")

# [UNCHANGED: All preprocessing logic stays the same]
df = clean_data(df)
df = add_feature_buckets(df)
df = add_interaction_features(df)
X, y, encoders = encode_and_normalize(df)

# === MODIFIED: Write to Azure Blob ===
cleaned_data = pd.DataFrame({"features": X.tolist(), "target": y.tolist()})
cleaned_csv = cleaned_data.to_csv(index=False)
processed_container.upload_blob(
    name="cleaned_data.csv",
    data=cleaned_csv,
    overwrite=True
)
logger.info("‚úÖ Uploaded cleaned_data.csv to Azure")

# === MODIFIED: Store encoders in Blob ===
encoders_bytes = pickle.dumps(encoders)
processed_container.upload_blob(
    name="encoders.pkl",
    data=encoders_bytes,
    overwrite=True
)
logger.info("‚úÖ Uploaded encoders.pkl to Azure")
```

#### New `requirements.txt` additions
```
azure-storage-blob==12.18.0
azure-identity==1.14.0
azure-monitor-opentelemetry==1.0.0
```

**What STAYS unchanged:**
- All preprocessing functions (expand_topic_category, clean_data, etc.)
- Feature engineering logic (buckets, interactions)
- LabelEncoder / StandardScaler usage

**What CHANGES:**
- Input: Local CSV ‚Üí Azure Blob read
- Output: Local files ‚Üí Azure Blob write
- Logging: print() ‚Üí Application Insights logger
- Auth: File system ‚Üí Azure Managed Identity

**Deployment:** Run as Azure Container Instance on schedule (via Data Factory)

---

### üìÑ FILE 2: `train_model_clean.py`

#### Current State (LOCAL)
```python
df = pd.read_csv("data/processed/cleaned_data.csv")
# Train 4 models...
pickle.dump(model, open("models/model.pkl", "wb"))
# Save metrics to text file
```

#### Migration Changes Required

| Change | Reason | Implementation |
|--------|--------|-----------------|
| Data input | Read from Azure, not local | Azure Blob read |
| MLflow tracking | Experiment versioning | MLflow on Azure ML backend |
| Metrics logging | Reproducibility | Log to MLflow UI |
| Model output | Model Registry, not local | Register in Azure ML Model Registry |
| Hyperparameters | Audit trail | Log all params to MLflow |

#### Modified `train_model_clean.py` (Azure-ready)
```python
from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential
import mlflow
import mlflow.sklearn
from mlflow.sklearn import autolog

# === NEW: MLflow setup ===
mlflow.set_tracking_uri("azureml://")  # Azure ML backend
mlflow.set_experiment("social-media-engagement")
mlflow.sklearn.autolog()  # Auto-log sklearn metrics

credential = DefaultAzureCredential()
blob_service = BlobServiceClient(
    account_url="https://{STORAGE_ACCOUNT}.blob.core.windows.net",
    credential=credential
)
processed_container = blob_service.get_container_client("processed-data")
models_container = blob_service.get_container_client("models")

# === MODIFIED: Read from Azure Blob ===
blob_client = processed_container.get_blob_client("cleaned_data.csv")
with open("temp_cleaned.csv", "wb") as f:
    f.write(blob_client.download_blob().readall())
df = pd.read_csv("temp_cleaned.csv")

# === MODIFIED: Load encoders from Azure Blob ===
encoders_blob = processed_container.get_blob_client("encoders.pkl")
encoders_bytes = encoders_blob.download_blob().readall()
encoders = pickle.loads(encoders_bytes)

# === UNCHANGED: Prepare data ===
X, y = prepare_data(df)
X_train, X_test, y_train, y_test = split_data(X, y)

# === NEW: Start MLflow run ===
with mlflow.start_run(run_name="full-training-pipeline"):
    
    # === MODIFIED: Log parameters ===
    params = {
        "train_size": len(X_train),
        "test_size": len(X_test),
        "n_features": X.shape[1]
    }
    mlflow.log_params(params)
    
    # === UNCHANGED: Train models (but logged automatically by autolog) ===
    models = {}
    for name in ["RandomForest", "ExtraTrees", "HistGradientBoosting", "XGBoost"]:
        model, best_params, mae = _search(...)
        models[name] = model
        
        # === NEW: Log model-specific metrics ===
        metrics = evaluate(model, X_train, y_train, X_test, y_test)
        mlflow.log_metrics({
            f"{name}_mae_train": metrics["mae_train"],
            f"{name}_mae_test": metrics["mae_test"],
            f"{name}_rmse_test": metrics["rmse_test"],
            f"{name}_r2_test": metrics["r2_test"]
        })
    
    # === UNCHANGED: Select best ===
    best_model = select_best_model(models)
    
    # === MODIFIED: Register model in Azure ML ===
    mlflow.sklearn.log_model(best_model, artifact_path="model")
    
    # === NEW: Register in Model Registry ===
    model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
    mv = mlflow.register_model(model_uri, "engagement-predictor")
    print(f"‚úÖ Model registered: {mv.name} v{mv.version}")
```

#### New requirements additions
```
mlflow==2.9.0
azure-ai-ml==1.13.0
```

**What STAYS unchanged:**
- All 4 model training logic
- Hyperparameter search (RandomizedSearchCV)
- Evaluation metrics (MAE, RMSE, R¬≤)
- Data splitting and preparation

**What CHANGES:**
- Input: Local CSV ‚Üí Azure Blob read
- Tracking: No tracking ‚Üí MLflow experiments (visible in Azure ML Studio)
- Output: Local pkl file ‚Üí Model Registry (versioned, reproducible)
- Metrics: Local text file ‚Üí MLflow metrics dashboard

**Deployment:** Run as Azure ML Job (scheduled or on-demand)

---

### üìÑ FILE 3: `app_clean.py`

#### Current State (LOCAL)
```python
model = pickle.load(open("models/model.pkl", "rb"))
encoders = pickle.load(open("data/processed/encoders.pkl", "rb"))
# Streamlit app on local machine
```

#### Migration Strategy: TWO OPTIONS

**Option A: Azure ML Real-Time Endpoint (RECOMMENDED)**
- Deploy model as scoring service
- Call via HTTP REST API
- No need for app container
- **Pros:** Simple, scalable, serverless
- **Cons:** Extra service call latency (~200ms)

**Option B: Streamlit on Azure Container Apps**
- Containerize Streamlit app
- Deploy to Container Apps (free tier)
- **Pros:** Full UI control, customizable
- **Cons:** More infrastructure

**I'll provide Option A (simpler for academic grading):**

#### Modified `app_clean.py` (Azure-ready)
```python
import streamlit as st
import requests
import json
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
import logging

# === NEW: Azure Key Vault client ===
credential = DefaultAzureCredential()
kv_client = SecretClient(
    vault_url="https://{KEY_VAULT_NAME}.vault.azure.net/",
    credential=credential
)

# === NEW: Get secrets from Key Vault ===
ML_ENDPOINT = kv_client.get_secret("MLEndpointURL").value
ML_API_KEY = kv_client.get_secret("MLEndpointKey").value

# Configure logging
logger = logging.getLogger(__name__)

@st.cache_resource
def get_model_info():
    """Get model metadata from Azure ML."""
    headers = {
        "Authorization": f"Bearer {ML_API_KEY}",
        "Content-Type": "application/json"
    }
    # Could add endpoint metadata call here
    return {"model": "engagement-predictor-v1"}

def predict_engagement(features_dict):
    """Call Azure ML endpoint for prediction."""
    headers = {
        "Authorization": f"Bearer {ML_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = json.dumps(features_dict)
    
    try:
        response = requests.post(
            f"{ML_ENDPOINT}/score",
            headers=headers,
            data=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        logger.info(f"‚úÖ Prediction: {result}")
        return result
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Endpoint error: {e}")
        st.error("Model service unavailable. Try again later.")
        return None

# === UNCHANGED: All UI functions ===
def show_welcome():
    # [Same as before]

def get_user_inputs(encoders):
    # [Same as before]

def get_engagement_level(rate):
    # [Same as before]

def get_recommendations(platform, engagement_level):
    # [Same as before]

def main():
    st.set_page_config(page_title="üì± Engagement Predictor", layout="wide")
    
    show_welcome()
    
    with st.spinner("Loading model..."):
        model_info = get_model_info()
    
    st.sidebar.info(f"Model: {model_info['model']}")
    
    # === MODIFIED: Collect inputs (no local encoding) ===
    inputs = get_user_inputs()
    
    if st.button("üöÄ Predict Engagement"):
        # === MODIFIED: Call Azure ML endpoint ===
        prediction = predict_engagement(inputs)
        
        if prediction:
            engagement_rate = prediction["engagement_rate"]
            level, desc, color = get_engagement_level(engagement_rate)
            
            st.success(f"{level}: {desc}")
            st.write(f"Predicted engagement: **{engagement_rate:.1%}**")
            
            recommendations = get_recommendations(inputs["platform"], level)
            st.info("\n".join(recommendations))
            
            # === NEW: Log prediction to database ===
            log_prediction_to_db({
                "input": inputs,
                "prediction": engagement_rate,
                "level": level
            })

if __name__ == "__main__":
    main()
```

**What STAYS unchanged:**
- All UI components (sidebar, buttons, sliders)
- Feature bucketing and encoding logic
- Recommendation generation
- Styling and layout

**What CHANGES:**
- Model loading: Local pkl ‚Üí Azure ML endpoint HTTP call
- Secrets: Hardcoded ‚Üí Azure Key Vault
- Encoders: Loaded locally ‚Üí Encoded by endpoint
- Error handling: Add retry logic for network calls

**Deployment Options:**
- **Option A (Simple):** Just update endpoint URL, deploy app elsewhere
- **Option B (Full Cloud):** Containerize and deploy to Azure Container Apps

**Talking Point:** "I migrated the app to use Azure ML inference endpoints, eliminating dependency on local files and enabling multi-region deployment."

---

## Summary of Changes

| File | Local Behavior | Azure Behavior | Key Change |
|------|---|---|---|
| preprocess_clean.py | Read CSV ‚Üí Write CSV | Read Blob ‚Üí Write Blob | Data ingestion layer |
| train_model_clean.py | Train ‚Üí Save local model | Train ‚Üí Register Model Registry | Experiment tracking + versioning |
| app_clean.py | Load local model ‚Üí Infer | Call REST endpoint ‚Üí Infer | Scalable inference |

